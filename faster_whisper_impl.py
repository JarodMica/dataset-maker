import os
import tkinter as tk
from tkinter import filedialog
from pathlib import Path
import numpy as np
import librosa
import soundfile as sf
import pysrt
import gc
import time

# Import the Slicer class from slicer2.py
from slicer2 import Slicer

# Import faster_whisper
from faster_whisper import WhisperModel


def load_faster_whisper_model(model_size="large-v3", device="cuda", compute_type="float16"):
    """
    Load and return the faster-whisper model.
    Options:
      - device: "cuda" or "cpu"
      - compute_type: "float16" (or other supported types like "int8", "int8_float16")
    """
    return WhisperModel(model_size, device=device, compute_type=compute_type)


def format_timestamp(seconds):
    """
    Convert seconds (float) to SRT timestamp format: HH:MM:SS,mmm.
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    milliseconds = int((seconds - int(seconds)) * 1000)
    return f"{hours:02}:{minutes:02}:{secs:02},{milliseconds:03}"


def write_srt(segments, output_path):
    """
    Write the transcription segments to an SRT file.
    
    Each segment is expected to have .start, .end, and .text attributes.
    """
    with open(output_path, "w", encoding="utf-8") as f:
        for i, segment in enumerate(segments, start=1):
            start_ts = format_timestamp(segment.start)
            end_ts = format_timestamp(segment.end)
            f.write(f"{i}\n")
            f.write(f"{start_ts} --> {end_ts}\n")
            f.write(f"{segment.text.strip()}\n\n")


def run_faster_whisper_transcription(audio_path, output_dir, language="en", beam_size=5, model=None):
    """
    Run transcription on the given audio file using faster-whisper.
    
    - The model transcribes the audio and returns segments plus language info.
    - The detected language and its probability are printed.
    - A custom SRT file is written to the specified output directory.
    - Returns the path to the generated SRT file.
    """
    # Transcribe using faster-whisper.
    segments, info = model.transcribe(str(audio_path), beam_size=beam_size)
    
    print("Detected language '%s' with probability %f" % (info.language, info.language_probability))
    
    # Optionally, you could check if the detected language matches the expected language.
    # For now, we assume language is either provided or the detected language is acceptable.
    
    # Determine output SRT file path.
    srt_path = Path(output_dir) / "transcription.srt"
    
    # Write the SRT file.
    write_srt(segments, srt_path)
    
    if not srt_path.exists():
        raise FileNotFoundError("No SRT file generated by faster-whisper.")
    
    return srt_path


def stitch_segments(segment_files, sr, silence_duration_sec=10):
    """
    Stitch segments from the list of WAV files by concatenating their NumPy arrays with a silent gap in between.
    Returns the stitched NumPy array.
    """
    # Sort files numerically (e.g. seg1.wav, seg2.wav, ...)
    segment_files = sorted(segment_files, key=lambda x: int(x.stem.replace("seg", "")))
    
    # Load the first segment to determine shape.
    first_data, _ = sf.read(str(segment_files[0]), dtype='float32')
    if first_data.ndim == 1:
        silence = np.zeros(int(sr * silence_duration_sec), dtype=np.float32)
    else:
        num_channels = first_data.shape[1]
        silence = np.zeros((int(sr * silence_duration_sec), num_channels), dtype=np.float32)
    
    stitched = []
    for seg_file in segment_files:
        data, _ = sf.read(str(seg_file), dtype='float32')
        stitched.append(data)
        stitched.append(silence)
    if stitched:
        stitched = stitched[:-1]  # Remove the final silence gap
    return np.concatenate(stitched)


def map_srt_to_segments(srt_file, seg_boundaries):
    """
    Given an SRT file (for the stitched audio) and a list of (start, end) times (in seconds)
    for each segment, assign each subtitle to the segment whose midpoint is closest to its start time.
    Returns a list of transcript strings (one per segment).
    """
    subs = pysrt.open(str(srt_file))
    transcripts = ["" for _ in seg_boundaries]
    # Compute midpoints for each segment boundary.
    seg_midpoints = [(start + end) / 2 for start, end in seg_boundaries]
    
    for sub in subs:
        start_time = (sub.start.hours * 3600 +
                      sub.start.minutes * 60 +
                      sub.start.seconds +
                      sub.start.milliseconds / 1000.0)
        # Find the index whose midpoint is closest to this subtitle's start time.
        closest_idx = min(range(len(seg_midpoints)), key=lambda i: abs(seg_midpoints[i] - start_time))
        transcripts[closest_idx] += " " + sub.text
    # Clean up extra whitespace.
    return [t.strip() for t in transcripts]


def process_audio_file(audio_file, model, output_base, train_txt_path, silence_duration_sec=3,
                       slicer_params=None):
    """
    Process one audio file:
      - Create a subfolder under output_base.
      - Load the audio and use slicer2.py's Slicer to split it.
      - Save each segment and record its duration.
      - Stitch segments together (with a silence gap) and run faster-whisper on the stitched audio.
      - Compute segment boundaries based on durations.
      - Map the SRT subtitles to segments using the midpoint algorithm.
      - Append one train.txt entry per segment.
    """
    print(f"Processing {audio_file}...")
    subfolder = output_base / audio_file.stem
    subfolder.mkdir(parents=True, exist_ok=True)
    
    # Load audio using librosa.
    y, sr = librosa.load(str(audio_file), sr=None, mono=False)
    
    # Set slicer parameters if not provided.
    if slicer_params is None:
        slicer_params = {
            'sr': sr,
            'threshold': -40.0,
            'min_length': 7000,    # milliseconds
            'min_interval': 1000,  # milliseconds
            'hop_size': 20,        # milliseconds
            'max_sil_kept': 500    # milliseconds
        }
    slicer = Slicer(**slicer_params)
    
    segments_np = slicer.slice(y)
    if not segments_np:
        print(f"No segments found for {audio_file}. Skipping.")
        return

    seg_durations = []
    segment_files = []
    for i, seg in enumerate(segments_np):
        seg_filename = subfolder / f"seg{i+1}.wav"
        if seg.ndim > 1:
            duration = seg.shape[1] / sr
            seg_to_write = seg.T  # Transpose multi-channel data
        else:
            duration = len(seg) / sr
            seg_to_write = seg
        seg_durations.append(duration)
        sf.write(str(seg_filename), seg_to_write, sr)
        print(f"Saved segment {i+1} to {seg_filename}")
        segment_files.append(seg_filename)
    
    # Stitch the segments with a silence gap.
    stitched_array = stitch_segments(segment_files, sr, silence_duration_sec)
    stitched_path = subfolder / "stitched.wav"
    sf.write(str(stitched_path), stitched_array, sr)
    print(f"Saved stitched audio: {stitched_path}")
    
    # Create a directory for faster-whisper outputs.
    whisper_out_dir = subfolder / "whisper_output"
    whisper_out_dir.mkdir(exist_ok=True)
    
    # Run faster-whisper transcription.
    srt_file = run_faster_whisper_transcription(stitched_path, whisper_out_dir, language="en",
                                                beam_size=5, model=model)
    
    # Compute segment boundaries in the stitched audio.
    seg_boundaries = []
    current_time = 0.0
    for duration in seg_durations:
        seg_boundaries.append((current_time, current_time + duration))
        current_time = current_time + duration + silence_duration_sec
    # Map SRT subtitles to segments.
    segment_transcripts = map_srt_to_segments(srt_file, seg_boundaries)
    
    # Write entries to train.txt.
    with train_txt_path.open("a", encoding="utf-8") as f:
        for i, transcript in enumerate(segment_transcripts):
            seg_filename = f"seg{i+1}.wav"
            f.write(f"{seg_filename} | {transcript}\n")
            print(f"Added dataset entry for {seg_filename}")


def main():
    # Let the user select the folder containing long audio files.
    root = tk.Tk()
    root.withdraw()
    folder_selected = filedialog.askdirectory(title="Select Folder with Audio Files")
    if not folder_selected:
        print("No folder selected. Exiting.")
        return
    
    audio_dir = Path(folder_selected)
    
    # Instead of writing to the chosen folder, output to a folder named output_{suffix} in the current directory.
    suffix = input("Enter output suffix (for output_{suffix} folder): ").strip() or "processed"
    start = time.time()
    output_base = Path.cwd() / f"output_{suffix}"
    output_base.mkdir(parents=True, exist_ok=True)
    
    # Define the path to the dataset file (train.txt)
    train_txt_path = output_base / "train.txt"
    
    print("Loading faster-whisper model (large-v3)...")
    model = load_faster_whisper_model("large-v3", device="cuda", compute_type="float16")
    
    audio_extensions = (".wav", ".mp3", ".m4a", ".opus", ".webm", ".mp4")
    for audio_file in audio_dir.iterdir():
        if audio_file.suffix.lower() in audio_extensions:
            try:
                process_audio_file(audio_file, model, output_base, train_txt_path, silence_duration_sec=3)
            except Exception as e:
                print(f"Error processing {audio_file}: {e}")
            gc.collect()

    print(f"Dataset creation complete. See {train_txt_path}")
    end = time.time()
    print(f"Total Time: {end-start}")


if __name__ == "__main__":
    main()
